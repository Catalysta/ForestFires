---
title: "forestfires_analysis"
author: "Christina Sousa"
date: "April 3, 2019"
output: html_document
---

Subset the data to train the model, reserving 1/4 for validation. Use same indices as those used in logistic regression.

```{r}
#please run all chunks from EDA prior to running this document.

#Check VIF since data appears to present with high multicollinearity

#set up regression models

cts.pred<-forest6[,6:12]
q<-ncol(cts.pred)
r2k<-numeric(q)
for (i in 1:q){
  formula<-as.formula(paste(colnames(cts.pred)[i],"~."))
  model<-lm(formula,data = cts.pred)
  r2k[i]<-summary(model)$r.squared
}
vifk<-1/(1-r2k)
vifk #these actually don't look too bad (but not great either)

#regress DMC on DC
summary(lm(DMC~DC, dat=forest6))$r.squared #actually not too bad, ok to leave in.

#check cook's distances
cd<-cooks.distance(lm(Larea~.,dat=forest6))
which(cd>qf(0.5,ncol(forest6)+1,nrow(forest6)-ncol(forest6)-1)) #these also look fine

```


```{r}

#add quadratic, cubic, and interaction terms
quad<-(forest6[,6:12])^2
labels.quad<-colnames(forest6[,6:12])
labels.quad<-paste(labels.quad,"^2")
colnames(quad)<-labels.quad
cubic<-(forest6[,6:12])^3
labels.cubic<-colnames(forest6[,6:12])
labels.cubic<-paste(labels.cubic,"^3")
colnames(cubic)<-labels.cubic

two.way.interaction<-matrix(nrow=nrow(forest6),ncol=21)
labels2<-vector()
#create interaction terms
k=1
for(i in 6:11){
  for (j in (i+1):12){
    two.way.interaction[,k]<-forest6[,i]*forest6[,j]
    labels2[k]<-paste(colnames(forest6)[i],colnames(forest6)[j], sep = "*")
    k<-(k+1)
  }
}
colnames(two.way.interaction)<-labels2

three.way.interaction<-matrix(nrow=nrow(forest6),ncol=35)
labels<-vector()
#create interaction terms
m=1
for(i in 6:10){
  for (j in (i+1):11){
    for (k in (j+1):12){
      three.way.interaction[,m]<-forest6[,i]*forest6[,j]*forest6[,k]
      labels[m]<-paste(colnames(forest6)[i],colnames(forest6)[j],colnames(forest6)[k], sep = "*")
      m<-(m+1)
    }
  }
}
colnames(three.way.interaction)<-labels

forest6.augmented<-cbind(forest6,quad,cubic,two.way.interaction,three.way.interaction)
```

```{r}
set.seed(6950)
indices<-sample(1:270,203)
train<-forest6.augmented[indices,]
test<-forest6.augmented[-indices,]
pairs(train[,6:13])
```



```{r}

#try LASSO on augmented prediction set
require(glmnet)
X<-model.matrix(Larea~.-1,data=train)
y<-train$Larea

cvfit<-cv.glmnet(X, y, nfolds=5)
plot(cvfit)
coef(cvfit, s = "lambda.min")

fit.lasso<-glmnet(X,y,alpha=1)
plot(fit.lasso,xvar="lambda",label=TRUE)

```

Since p is small, we enumerate full model space using `leaps` package.

```{r}
require(leaps)
y<-train$Larea
X<-train[,c(1:12,14:15)]
regsubsets.out=regsubsets(y ~ X,
			   data = as.data.frame(cbind(y,X)),
               nbest = 2,       # 2 best models for each number of predictors
               nvmax = NULL,    # NULL for no limit on number of variables
               force.in = NULL, force.out = NULL,
               method = "exhaustive") # or "forward" for forward selection,
                                      # "backward" for backward elimination
regsubsets.out
sum.reg=summary(regsubsets.out)
sum.reg

```

`DMC` appears to be least important, while `month` and `day` appear to be important. R^2 is extremely low. This data simply does not seem to predict `Larea` very well.

```{r}
par(mfrow=c(1,3))
plot(regsubsets.out, scale = "r2", main = "R^2",cex.axis=2,cex.lab=2,cex=2)
plot(regsubsets.out, scale = "adjr2", main = "Adjusted R^2",cex.axis=2,cex.lab=2,cex=2)
plot(regsubsets.out, scale = "Cp", main = "Mallow's Cp",cex.axis=2,cex.lab=2,cex=2)
```





