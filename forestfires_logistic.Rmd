---
title: "forestfires_logistic"
author: "Christina Sousa"
date: "April 4, 2019"
output: html_document
---

## Introduction

## Data

The variables in the data are given by:

   1. **X** *(nominal)*: x-axis spatial coordinate within the Montesinho park map: 1 to 9
   2. **Y** *(nominal)*: y-axis spatiFal coordinate within the Montesinho park map: 2 to 9
   3. **month** *(nominal)*: month of the year: "jan" to "dec" 
   4. **day** *(nominal)*: day of the week: "mon" to "sun"
   5. **FFMC** *(ordinal, continuous)*: FFMC index from the FWI system: 18.7 to 96.20
   6. **DMC** *(ordinal, continuous)*: DMC index from the FWI system: 1.1 to 291.3 
   7. **DC** *(ordinal, continuous)*: DC index from the FWI system: 7.9 to 860.6 
   8. **ISI** *(ordinal, continuous)*: ISI index from the FWI system: 0.0 to 56.10
   9. **temp** *(interval, continuous)*: temperature in Celsius degrees: 2.2 to 33.30
   10. **RH** *(ordinal, discrete)*: relative humidity in %: 15.0 to 100
   11. **wind** *(ratio, continuous)* wind speed in km/h: 0.40 to 9.40 
   12. **rain** *(ratio, discrete)*: outside rain in mm/m2 : 0.0 to 6.4 
   13. **area** *(ratio, continuous)*: the burned area of the forest (in ha): 0.00 to 1090.84 

```{r}
#data file can be obtained at https://archive.ics.uci.edu/ml/machine-learning-databases/forest-fires/
#read data into r
forest7 <-read.csv("forestfires.csv")

# obs 23 looks influential but the model is worse when you erase it
# forest7 <- forest7[-23,]
forest7$area <- log(forest7$area + 1)


#change spatial variables X and Y to factors
forest7$XY <- 10*forest7$X+forest7$Y
forest7$X<-as.factor(forest7$X)
forest7$Y<-as.factor(forest7$Y)
forest7$XY<-as.factor(as.integer(forest7$XY))
forest7<-forest7[,c(14,1:13)]
#add binomial area column
forest8<-cbind(forest7,forest7$area>0)
colnames(forest8)[15]<-"large"
forest8$large<-as.factor(forest8$large)

#add categorical rain column
forest9<-cbind(forest8,forest8$rain>0)
colnames(forest9)[16]<-"rainbin"
forest9$rain<-as.factor(forest9$rain)

#add categorical weekday and season variables
week <- rep(0,length(forest9$day))

week[which(forest9$day == "mon")] <- 0
week[which(forest9$day == "tue")] <- 0
week[which(forest9$day == "wed")] <- 0
week[which(forest9$day == "thu")] <- 0
week[which(forest9$day == "fri")] <- 1
week[which(forest9$day == "sat")] <- 1
week[which(forest9$day == "sun")] <- 1

seas <- rep(0,length(forest9$month), ylab = "log(area)",
     xlab = "Month",main = "Month Data")

seas[which(forest9$month == "jan")] <- 0
seas[which(forest9$month == "feb")] <- 0
seas[which(forest9$month == "mar")] <- 0
seas[which(forest9$month == "apr")] <- 1
seas[which(forest9$month == "may")] <- 1
seas[which(forest9$month == "jun")] <- 1
seas[which(forest9$month == "jul")] <- 2
seas[which(forest9$month == "aug")] <- 2
seas[which(forest9$month == "sep")] <- 2
seas[which(forest9$month == "oct")] <- 3
seas[which(forest9$month == "nov")] <- 3
seas[which(forest9$month == "dec")] <- 3

forest9$week <- as.factor(week)
levels(forest9$week)<-c("weekday","weekend")
forest9$season <- as.factor(seas)
levels(forest9$season)<-c("Winter","Spring","Summer","Fall")
```

```{r}
#clean up other variables

forest9$month = factor(forest9$month,
                       levels = levels(forest9$month)[c(5,                                                        4,8,1,9,7,6,2,12,11,10,3)])
forest9$day = factor(forest9$day,
                     levels = 
                       levels(forest9$day)[c(2,6,7,5,1,3,4)])

#pairs(forest9)
forest9 <- forest9[,c(1:13,16:18,14,15)]
pairs(forest9[,c(-2,-3,-4,-5)])


```

```{r}
#split training and testing set, using approximately 75/25 ratio train/test

set.seed(6950)

indices<-sample(1:517,388)
train2<-forest9[indices,]
test2<-forest9[-indices,]

train3<-train2[,c(-(1:3),-13,-(15:17))]

#view continuous predictors
pairs(train3[,3:9])

#transform FFMC
train4<-train3
train4$FFMC <- log(-train4$FFMC+ max(train4$FFMC)+1)
colnames(train4)[3]<-"TrFFMC"

#view continuous predictors TrFFMC
pairs(train2[,c(6:12)])

pairs(train4[,3:9])

#check cook's distances
cd<-cooks.distance(glm(large~.,dat=train4,family="binomial"))
which(cd>qf(0.5,ncol(train2)+1,nrow(train2)-ncol(train2)-1)) #these look fine

#use deviance as a measure instead
#function for deviance calcs
dev.distance<-function(dat,response,model){
  n<-nrow(dat)
  output<-data.frame(aic=numeric(),aic.dist=numeric(),dev=numeric(),dev.dist=numeric())
  full.model<-glm(model$call, dat=dat, family = "binomial")
  full.model.aic<-full.model$aic
  full.model.dev<-full.model$deviance
  for (i in 1:n){
    temp<-dat[-i,]
    oo.mod<-glm(model$call, dat=temp, family = "binomial")
    output[i,1]<-oo.mod$aic
    output[i,2]<-abs(output[i,1]-full.model.aic)
    output[i,3]<-oo.mod$deviance
    output[i,4]<-abs(output[i,3]-full.model.dev)
  }
  return(output)
}

#run function on train4
model1<-glm(large~.,dat=train4,family="binomial")
result<-dev.distance(train4,train4$large,model=model1)
hist(result$aic.dist)
hist(result$dev.dist)
#get quantiles
aic.q<-quantile(result$aic.dist,.99)
dev.q<-quantile(result$dev.dist,.99)
ind1<-which(result$aic.dist>aic.q)
ind2<-which(result$dev.dist>dev.q)
color<-ifelse(row.names(train4) %in% ind2, "red", "black")
pairs(train4[,3:9],col=color)
result[ind2,]

#even though there are outliers relative to the rest of the data set, on a relative
#scale these obervations do not appear to change the model very much
#so even though visually the outlier in ISI looks bad, we have no analytical
#justification for removing it.

#pairs(train5[,3:9])
#remove outlier
#train5<-train4[which(train3$ISI<40),]
#pairs(train5[,3:9])

#since the plots look good and box-cox does not suggest any further drastic transformations,
#we proceed with variable selection
require(car)
summary(powerTransform(cbind(TrFFMC,DMC,DC,ISI,temp,RH,wind)~1,train4,family="yjPower"))

```

## Model Selection

```{r}
fit0<-glm(large~1,dat=train4, family = binomial(link="logit"))
fitfull<-glm(large~.,dat=train4, family = binomial(link="logit"))

# Forward selection
step(fit0,scope=list(lower=fit0,upper=fitfull),direction="forward")

# Backward elimination
step(fitfull,scope=list(lower=fit0,upper=fitfull),direction="backward")

# Stepwise regression
step(fit0,scope=list(lower=fit0,upper=fitfull),direction="both")

#compare resulting models
fw.model<-glm(formula = large ~ month + temp, family = binomial(link = "logit"), 
    data = train4)
stepwise.model<-glm(formula = large ~ month + temp + RH + wind + rainbin, family = binomial(link = "logit"), 
    data = train4)

summary(fw.model)
summary(stepwise.model) #this model looks better in terms of deviance

#perform chi-squared test
stat<-fw.model$deviance-stepwise.model$deviance
qchisq(.95,2)
#[1] 5.991465
stat
#[1] 6.643096 #Hence adding the variables in the stepwise model significantly improves performance in terms of deviance

```

```{r, warning = F, message=F}
#LASSO with everything in
require(glmnet)
X<-model.matrix(large~.-1,data=train4)
y<-train4$large

cvfit3<-cv.glmnet(X, y, family = "binomial", nfolds=5)
plot(cvfit3)
coef(cvfit3, s = "lambda.min")

fit.lasso<-glmnet(X,y,alpha=1, family="binomial")
plot(fit.lasso,xvar="lambda",label=TRUE)

#Fit Ridge
cvfit.ridge<-cv.glmnet(X, y, family = "binomial", nfolds=5, alpha = 0)
plot(cvfit.ridge)
coef(cvfit.ridge, s = "lambda.min")

fit.ridge<-glmnet(X,y,alpha=0, family="binomial")
plot(fit.ridge,xvar="lambda",label=TRUE)

#Fit Elastic Net
cvfit.elastic<-cv.glmnet(X, y, family = "binomial", nfolds=5, alpha = 0.5)
plot(cvfit.elastic)
coef(cvfit.elastic, s = "lambda.min")

fit.elastic<-glmnet(X,y,alpha=0.5, family="binomial")
plot(fit.elastic,xvar="lambda",label=TRUE)


#exclude month and day and see what LASSO says
X2<-model.matrix(large~TrFFMC+DMC+DC+ISI+temp+RH+wind+rainbin-1,data=train4)

cvfit5<-cv.glmnet(X2, y, family = "binomial", nfolds=5)
plot(cvfit5)
coef(cvfit5, s = "lambda.min")

fit.lasso2<-glmnet(X2,y,alpha=1, family="binomial")
plot(fit.lasso2,xvar="lambda",label=TRUE)

#exclude month, day and DMC and see what LASSO says
X4<-model.matrix(large~TrFFMC+DC+ISI+temp+RH+wind+rainbin-1,data=train4)

cvfit7<-cv.glmnet(X4, y, family = "binomial", nfolds = 5) 
plot(cvfit7)
coef(cvfit7, s = "lambda.min")

fit.lasso4<-glmnet(X4,y,alpha=1, family="binomial")
plot(fit.lasso4,xvar="lambda",label=TRUE)


```


```{r, echo=F}

# mylogit<-glm(formula = large ~ temp+RH+DC+ISI+DMC, family = "binomial", data = train5)
# summary(mylogit)


#mylogit<-glm(formula = large ~ XY+week+season+rain+
#               wind+FFMC+temp+RH+DC+ISI+DMC,
#             family = "binomial", data = train5)
#summary(mylogit)


#mylogit<-glm(formula = large ~ XY+week+season+rainbin+
#               wind+FFMC+temp+RH+DC+ISI+DMC,
#             family = "binomial", data = train5)
#summary(mylogit)


#mylogit<-glm(formula = large ~ week+season+rainbin+
#               wind+FFMC+temp+RH+DC+ISI+DMC,
#             family = "binomial", data = train5)
#summary(mylogit)


#mylogit<-glm(formula = large ~ week+season+rain+
#               wind+FFMC+temp+RH+DC+ISI+DMC,
#             family = "binomial", data = train5)
#summary(mylogit)


#mylogit<-glm(formula = large ~ season+rainbin+
#               wind+FFMC+temp+RH+DC+ISI+DMC,
#             family = "binomial", data = train5)
#summary(mylogit)

# 
# mylogit<-glm(formula = large ~ wind+rainbin,
#              family = "binomial", data = train5)
# summary(mylogit)

```

### Model selection comparing all the models

When we compare the $2^10-1$ possible models using all the available variables, we get the following results on the deviances.

```{r}

min = matrix(rep(0,20),nrow = 10, ncol = 2)
dev.min = matrix(rep(1000,20),nrow = 10, ncol = 2)
devs <- rep(0,1023)


for (i in 1:1023) {
    train.x <- train4[,c(as.integer(intToBits(i))[1:10]==1,T)]
    
    mylogit<-glm(formula = large ~.,
                 family = "binomial", data = train.x)
    summary(mylogit)
    
    j = sum(as.integer(intToBits(i))[1:10])
    
    
    if(dev.min[j,2] > mylogit$deviance) {
        min[j,2] = i
        dev.min[j,2] = mylogit$deviance
    }
    
    
    if(dev.min[j,1] > dev.min[j,2]) {
      min[j,] = min[j,c(2,1)]
      dev.min[j,] = dev.min[j,c(2,1)]
    }
    
}


dev.min <- dev.min[,c(2,1)]

g <- rbind(1:10,1:10)
x.vec <- as.vector(g)
x.vec[2*(1:10)] <- x.vec[2*(1:10)]+0.1
x.vec[2*(1:10)-1] <- x.vec[2*(1:10)-1]-0.1
x.vec

devs <- as.vector(t(dev.min))
x.vec <- x.vec[c(-19)]
devs <- devs[c(-19)]

x.vec[1] <- 0
devs[1] <- mylogit$null.deviance
plot(x.vec,devs,type = "l",main = "Lowest Two Deviances vs. Number of Predictors",xlab = "Number of predictors", ylab = "Deviance",xaxt = "n")
axis(1, at = 0:10, labels = 0:10)
points(x.vec,devs,pch=18)

i = min[6,2]

```

The deviance seems to stabilize when using six predictors, and the resulting model uses the variables **month**, **day**, **TrFFMC**, **temp**, **RH**, and **rainbin**.

```{r}

train.x <- train4[,c(as.integer(intToBits(i))[1:10]==1,T)]

mylogit<-glm(formula = large ~.,
             family = "binomial", data = train.x)
summary(mylogit)

```
However, as we as are mostly interested in weather conditions, if we drop the **month** and **day** predictors a priori, we get the following result.

```{r}

train.4 <- train4[,c(-1,-2)]


min = matrix(rep(0,16),nrow = 8, ncol = 2)
dev.min = matrix(rep(1000,16),nrow = 8, ncol = 2)
devs <- rep(0,255)


for (i in 1:255) {
  train.x <- train.4[,c(as.integer(intToBits(i))[1:8]==1,T)]
  
  mylogit<-glm(formula = large ~.,
               family = "binomial", data = train.x)
  summary(mylogit)
  
  j = sum(as.integer(intToBits(i))[1:8])
  
  
  if(dev.min[j,2] > mylogit$deviance) {
    min[j,2] = i
    dev.min[j,2] = mylogit$deviance
  }
  
  
  if(dev.min[j,1] > dev.min[j,2]) {
    min[j,] = min[j,c(2,1)]
    dev.min[j,] = dev.min[j,c(2,1)]
  }
  
}


dev.min <- dev.min[,c(2,1)]

g <- rbind(1:8,1:8)
x.vec <- as.vector(g)
x.vec[2*(1:8)] <- x.vec[2*(1:8)]+0.1
x.vec[2*(1:8)-1] <- x.vec[2*(1:8)-1]-0.1
x.vec

devs <- as.vector(t(dev.min))
x.vec <- x.vec[c(-15)]
devs <- devs[c(-15)]

x.vec[1] <- 0
devs[1] <- mylogit$null.deviance

plot(x.vec,devs,type = "l",main = "Lowest Two Deviances vs. Number of Predictors",xlab = "Number of predictors", ylab = "Deviance",xaxt = "n")
axis(1, at = 0:8, labels = 0:8)
points(x.vec,devs,pch=18)

i = min[4,2]


train.x <- train.4[,c(as.integer(intToBits(i))[1:8]==1,T)]

mylogit<-glm(formula = large ~.,
             family = "binomial", data = train.x)
summary(mylogit)

```    

The deviance seemsto stabilize when the number of predictors is four, which are **DC**, **temp**, **wind**, and **rainbin**.

### Variable selection using classification trees

The classification tree reulted in three significant variables, **temp**,**RH**, and **DC**

```{r}
require(rpart)
forestx <- forest9
forestx$FFMC <- log(max(forestx$FFMC)-forestx$FFMC)
colnames(forestx)[6] <- "TrFFMC"

fit = rpart(large ~ rainbin+
              wind+TrFFMC+temp+RH+DC+ISI+DMC,
            dat=forestx,method="class",
            control = rpart.control(cp = 0.03))

#fit
plot(fit,margin = 0.1)
text(fit,use.n=TRUE,cex=0.7)
# 
# #printcp(fit)
# 
# #plotcp(fit)
# 
# 
# fit.pruned = prune(fit,cp=.03) 
# fit.pruned
# #summary(fit.pruned)
# plot(fit.pruned,margin = 0.1)
# text(fit.pruned,use.n=TRUE,cex=1.2)

mylogit<-glm(formula = large ~temp+DC+RH,
             family = "binomial", data = train4)
summary(mylogit)

```